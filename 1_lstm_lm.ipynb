{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_lm_master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYdn1woOS1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750b7250-d250-477d-a4b3-d8433d3f5edc"
      },
      "source": [
        "!pip install torch==1.9 torchtext==0.10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.9 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: torchtext==0.10 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV9Cis3HdHp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ac9e78-0dbf-4184-d40e-128542bdaa4f"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.10)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaD4Nk85FsZc"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1dT3x9ZFt-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f61d2f6-c70b-495d-8d65-36dee6d67abe"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8xZhIfJ9smN",
        "outputId": "a0a02d0d-ce24-4591-ec93-e77d47d8204e"
      },
      "source": [
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7471c80f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djyFDC4QFwVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505420cc-2678-42be-c170-e1f5492e5e04"
      },
      "source": [
        "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B_uOuSTF0PU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d791658-4d99-48e7-ed52-5c446d0c3204"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 4358\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 36718\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 3760\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5QwnBkTcfNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bee43de-0fa5-4462-9405-7410669b33df"
      },
      "source": [
        "dataset['train'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ''}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp-LECVmcgSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b6bfb3-abf5-44f9-e145-cd2465b4d77e"
      },
      "source": [
        "dataset['train'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' = Valkyria Chronicles III = \\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKcltvAsfgHW"
      },
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCJYUEGrfxo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9771a7-c09c-4f1d-d5fd-240733665653"
      },
      "source": [
        "tokenizer('hello world how are you?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world', 'how', 'are', 'you', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4ipPKSwf0jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b180fb8f-3981-4649-f0fa-59a4e1a2beef"
      },
      "source": [
        "tokenizer(dataset['train'][1]['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['=', 'valkyria', 'chronicles', 'iii', '=']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aODmmThhcpTx"
      },
      "source": [
        "def tokenize_data(example, tokenizer):\n",
        "    tokens = {'tokens': tokenizer(example['text'])}\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5x9lbSFc5Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9798a3d5-f4d9-4fef-d92d-d05b052ac5cf"
      },
      "source": [
        "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-38e31fad4a61d72e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-2181ba6714368d4f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-12708e5ca86f73dd.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGF9LYAiEqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b1fdfe-2a9f-4745-fb92-efa7e7c0cb68"
      },
      "source": [
        "tokenized_dataset['train'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens': ['=', 'valkyria', 'chronicles', 'iii', '=']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1NIY8yiHJo"
      },
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],\n",
        "                                                  min_freq=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9onJczdaiq1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd05069-0232-4803-a630-b83fdd8466b4"
      },
      "source": [
        "vocab.get_itos()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', ',', '.', 'of', 'and', 'in', 'to', 'a', '=', 'was']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzS26a0GIERG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff380691-0d5a-4660-ad01-8fb4e0df7155"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUSGIDkP_YvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ea85d8-abbf-49d5-e527-429aa79394f1"
      },
      "source": [
        "'hello' in vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54CbiCkw_5P3"
      },
      "source": [
        "vocab.insert_token('<unk>', 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTNbJipk_k5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac535b76-09f7-4a2d-eaf1-1be5ab0a563d"
      },
      "source": [
        "vocab.get_itos()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a', '=']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grwHG5UN_eKN"
      },
      "source": [
        "vocab.set_default_index(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_JUTdrJ_m7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08978606-d856-416c-e14e-074d1083f0e7"
      },
      "source": [
        "vocab['hello']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYKf-779yFCB"
      },
      "source": [
        "vocab.insert_token('<eos>', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtYdBoybyI9B",
        "outputId": "144c1aae-d75c-4172-cf6c-283c407fb671"
      },
      "source": [
        "vocab.get_itos()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUwATIrwkAi"
      },
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []\n",
        "    for example in dataset:\n",
        "        if example['tokens']:\n",
        "            tokens = example['tokens'].append('<eos>')\n",
        "            tokens = [vocab[token] for token in example['tokens']]\n",
        "            data.extend(tokens)\n",
        "    data = torch.LongTensor(data)\n",
        "    n_batches = data.shape[0] // batch_size\n",
        "    data = data.narrow(0, 0, n_batches * batch_size)\n",
        "    data = data.view(batch_size, -1)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtjtIvuNisST"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsmojuCbivzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45871921-68a3-4d1b-9d66-b377e70c9931"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 16214])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038OtCf3hbeY"
      },
      "source": [
        "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
        "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAKBc-UUjO0-"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate, tie_weights):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout_rate, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        if tie_weights:\n",
        "            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n",
        "            self.embedding.weight = self.fc.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range = 0.1\n",
        "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden, cell\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input = [batch size, seq len]\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        embedding = self.dropout(self.embedding(input))\n",
        "        # embedding = [batch size, seq len, embedding dim]\n",
        "        output, hidden = self.lstm(embedding, hidden)\n",
        "        # output = [batch size, seq len, hidden dim]\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        # output = [batch size, seq len, vocab size]\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYYhJ3WjWpY"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 1024\n",
        "hidden_dim = 1024\n",
        "n_layers = 2\n",
        "dropout_rate = 0.65\n",
        "tie_weights = True\n",
        "\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, n_layers, dropout_rate, tie_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C9nTPpxsclE",
        "outputId": "fc5389ad-fd5c-4aec-964c-72600428a3f0"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 47,003,425 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXwUDCA-92-y"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkggfxKJBZ04"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMfUy_H2IdUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0f6f8c-0a34-41b9-8687-c4b026f9f741"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoPhYH3PJB-S"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5omPylb8Kag_"
      },
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, max_seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    n_tokens = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for offset in range(0, n_tokens - 1, max_seq_len):\n",
        "        optimizer.zero_grad()\n",
        "        input, target, seq_len = get_batch(data, max_seq_len, n_tokens, offset)\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        # input = [batch size, seq len]\n",
        "        # target = [batch size, seq len]\n",
        "        batch_size, seq_len = input.shape\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        output, hidden = model(input, hidden)\n",
        "        # output = [batch size, seq len, vocab size]\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        output = output.reshape(batch_size * seq_len, -1)\n",
        "        target = target.reshape(-1)\n",
        "        # output = [batch size * seq len, vocab size]\n",
        "        # target = [batch size * seq len]\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / n_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iikSU_1YT0dV"
      },
      "source": [
        "def get_batch(data, max_seq_len, n_tokens, offset):\n",
        "    seq_len = min(max_seq_len, n_tokens - offset - 1)\n",
        "    input = data[:, offset:offset+seq_len]\n",
        "    target = data[:, offset+1:offset+seq_len+1]\n",
        "    return input, target, seq_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ev0pAlFMAgH"
      },
      "source": [
        "def evaluate(model, data, criterion, batch_size, max_seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    n_tokens = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for offset in range(0, n_tokens - 1, max_seq_len):\n",
        "            input, target, seq_len = get_batch(data, max_seq_len, n_tokens, offset)\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            # input = [batch size, seq len]\n",
        "            # target = [batch size, seq len]\n",
        "            batch_size, seq_len = input.shape\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            # hidden = [n layers, batch size, hidden dim]\n",
        "            output, hidden = model(input, hidden)\n",
        "            # output = [batch size, seq len, vocab size]\n",
        "            # hidden = [n layers, batch size, hidden dim]\n",
        "            output = output.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "            # output = [batch size * seq len, vocab size]\n",
        "            # target = [batch size * seq len]\n",
        "            loss = criterion(output, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / n_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_qDt7hWi3wJ"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q6owq2DwP9Y"
      },
      "source": [
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVEjus7MElw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55852428-2e30-401a-c250-0854caa09a0d"
      },
      "source": [
        "n_epochs = 50\n",
        "max_seq_len = 50\n",
        "clip = 0.25\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    train_loss = train(model, train_data, optimizer, criterion, batch_size, max_seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, max_seq_len, device)\n",
        "    \n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lstm_lm.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 623.137\n",
            "\tValid Perplexity: 279.474\n",
            "Epoch: 02 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 303.460\n",
            "\tValid Perplexity: 202.249\n",
            "Epoch: 03 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 222.573\n",
            "\tValid Perplexity: 168.837\n",
            "Epoch: 04 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 179.383\n",
            "\tValid Perplexity: 146.603\n",
            "Epoch: 05 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 152.362\n",
            "\tValid Perplexity: 137.059\n",
            "Epoch: 06 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 133.825\n",
            "\tValid Perplexity: 127.296\n",
            "Epoch: 07 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 120.237\n",
            "\tValid Perplexity: 120.745\n",
            "Epoch: 08 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 109.518\n",
            "\tValid Perplexity: 115.251\n",
            "Epoch: 09 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 100.898\n",
            "\tValid Perplexity: 112.620\n",
            "Epoch: 10 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 93.745\n",
            "\tValid Perplexity: 110.718\n",
            "Epoch: 11 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 88.096\n",
            "\tValid Perplexity: 108.855\n",
            "Epoch: 12 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 83.094\n",
            "\tValid Perplexity: 107.296\n",
            "Epoch: 13 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 78.756\n",
            "\tValid Perplexity: 106.687\n",
            "Epoch: 14 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 74.926\n",
            "\tValid Perplexity: 104.361\n",
            "Epoch: 15 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 71.699\n",
            "\tValid Perplexity: 104.082\n",
            "Epoch: 16 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 68.818\n",
            "\tValid Perplexity: 103.994\n",
            "Epoch: 17 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 66.408\n",
            "\tValid Perplexity: 103.884\n",
            "Epoch: 18 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 64.036\n",
            "\tValid Perplexity: 102.454\n",
            "Epoch: 19 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 61.971\n",
            "\tValid Perplexity: 102.674\n",
            "Epoch: 20 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 57.797\n",
            "\tValid Perplexity: 100.732\n",
            "Epoch: 21 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 55.863\n",
            "\tValid Perplexity: 101.289\n",
            "Epoch: 22 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 53.704\n",
            "\tValid Perplexity: 100.059\n",
            "Epoch: 23 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 52.689\n",
            "\tValid Perplexity: 99.763\n",
            "Epoch: 24 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 51.935\n",
            "\tValid Perplexity: 99.658\n",
            "Epoch: 25 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 51.186\n",
            "\tValid Perplexity: 99.547\n",
            "Epoch: 26 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 50.619\n",
            "\tValid Perplexity: 99.584\n",
            "Epoch: 27 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.643\n",
            "\tValid Perplexity: 99.638\n",
            "Epoch: 28 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.486\n",
            "\tValid Perplexity: 99.027\n",
            "Epoch: 29 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.246\n",
            "\tValid Perplexity: 98.958\n",
            "Epoch: 30 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.031\n",
            "\tValid Perplexity: 99.194\n",
            "Epoch: 31 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.196\n",
            "\tValid Perplexity: 98.686\n",
            "Epoch: 32 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 48.881\n",
            "\tValid Perplexity: 98.615\n",
            "Epoch: 33 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 48.674\n",
            "\tValid Perplexity: 98.623\n",
            "Epoch: 34 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 48.942\n",
            "\tValid Perplexity: 98.333\n",
            "Epoch: 35 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 48.851\n",
            "\tValid Perplexity: 98.364\n",
            "Epoch: 36 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.126\n",
            "\tValid Perplexity: 98.184\n",
            "Epoch: 37 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.214\n",
            "\tValid Perplexity: 98.128\n",
            "Epoch: 38 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.169\n",
            "\tValid Perplexity: 98.091\n",
            "Epoch: 39 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.357\n",
            "\tValid Perplexity: 98.025\n",
            "Epoch: 40 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.279\n",
            "\tValid Perplexity: 98.017\n",
            "Epoch: 41 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.470\n",
            "\tValid Perplexity: 97.985\n",
            "Epoch: 42 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.730\n",
            "\tValid Perplexity: 97.957\n",
            "Epoch: 43 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.691\n",
            "\tValid Perplexity: 97.937\n",
            "Epoch: 44 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.719\n",
            "\tValid Perplexity: 97.929\n",
            "Epoch: 45 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.791\n",
            "\tValid Perplexity: 97.925\n",
            "Epoch: 46 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.810\n",
            "\tValid Perplexity: 97.923\n",
            "Epoch: 47 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.809\n",
            "\tValid Perplexity: 97.921\n",
            "Epoch: 48 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.847\n",
            "\tValid Perplexity: 97.921\n",
            "Epoch: 49 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.848\n",
            "\tValid Perplexity: 97.920\n",
            "Epoch: 50 | Epoch Time: 1m 29s\n",
            "\tTrain Perplexity: 49.743\n",
            "\tValid Perplexity: 97.920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Y0oNdgRkad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf4590e-208d-478e-8e96-776222dee355"
      },
      "source": [
        "model.load_state_dict(torch.load('lstm_lm.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, max_seq_len, device)\n",
        "\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity: 93.684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GexY2GcCRtXu"
      },
      "source": [
        "def generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(0)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_gen_tokens):\n",
        "            input = torch.LongTensor([indices]).to(device)\n",
        "            output, hidden = model(input, hidden)\n",
        "            probs = torch.softmax(output[:, -1] / temperature, dim=-1) \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "            indices.append(prediction)\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R778iX9gR38L"
      },
      "source": [
        "prompt = 'the'\n",
        "n_gen_tokens = 25\n",
        "temperature = 0.5\n",
        "seed = 0\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1MCYs3mR7nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d1e524-7982-4539-b757-c88bf1e6a6d5"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'highest',\n",
              " '@-@',\n",
              " 'paid',\n",
              " 'of',\n",
              " 'the',\n",
              " 'year',\n",
              " '.',\n",
              " 'it',\n",
              " 'was',\n",
              " 'a',\n",
              " 'critical',\n",
              " 'success',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'first',\n",
              " 'two',\n",
              " '@-@',\n",
              " 'year',\n",
              " 'run',\n",
              " ',',\n",
              " 'the',\n",
              " 'first',\n",
              " 'time',\n",
              " 'in']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpz2dfdcR8mc"
      },
      "source": [
        "temperature = 0.1\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87N3LkyVpEIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4247d86b-9927-45c9-e7fa-24c74fd5bb3b"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " '<unk>',\n",
              " '<unk>',\n",
              " ',',\n",
              " 'which',\n",
              " 'was',\n",
              " 'the',\n",
              " 'first',\n",
              " 'to',\n",
              " 'be',\n",
              " 'built',\n",
              " 'in',\n",
              " 'the',\n",
              " '<unk>',\n",
              " '.',\n",
              " '<eos>',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " 'chapel',\n",
              " 'of',\n",
              " 'our',\n",
              " 'lady',\n",
              " 'of',\n",
              " 'our']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbGOerWrpHnc"
      },
      "source": [
        "temperature = 1.5\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZLB5jwgpKUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294be7f2-b391-49e0-fba1-fb1885a866d3"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'hide',\n",
              " 'swap',\n",
              " 'just',\n",
              " 'leads',\n",
              " 'landmarks',\n",
              " 'and',\n",
              " 'arranged',\n",
              " 'discussions',\n",
              " '3',\n",
              " 'agree',\n",
              " 'specifically',\n",
              " 'with',\n",
              " 'the',\n",
              " 'friend',\n",
              " 'harvest',\n",
              " 'as',\n",
              " 'captains',\n",
              " 'like',\n",
              " 'tom',\n",
              " 'bradley',\n",
              " 'giger',\n",
              " 'viewed',\n",
              " 'the',\n",
              " 'team',\n",
              " \"'\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYNUGi9uy-qr"
      },
      "source": [
        "temperature = 0.75\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAf98XY-9-T6",
        "outputId": "10e7b4a2-0454-489a-c542-5a65deb1783f"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'highest',\n",
              " '<unk>',\n",
              " 'in',\n",
              " 'the',\n",
              " 'united',\n",
              " 'states',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'a',\n",
              " 'oldman',\n",
              " 'city',\n",
              " ',',\n",
              " 'and',\n",
              " 'the',\n",
              " 'st',\n",
              " '.',\n",
              " 'louis',\n",
              " 'rail',\n",
              " 'district',\n",
              " 'has',\n",
              " 'a',\n",
              " 'population',\n",
              " 'of',\n",
              " '17']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clWCfGOF-N8G"
      },
      "source": [
        "temperature = 0.8\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJs4jw1n-N6D",
        "outputId": "8f8f57c2-1079-45aa-b9af-97c17d28e283"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'highest',\n",
              " 'swap',\n",
              " 'in',\n",
              " 'the',\n",
              " 'era',\n",
              " '.',\n",
              " 'the',\n",
              " 'old',\n",
              " '3',\n",
              " '@',\n",
              " '.',\n",
              " '@',\n",
              " '06',\n",
              " 'm',\n",
              " '(',\n",
              " '3',\n",
              " '@',\n",
              " '.',\n",
              " '@',\n",
              " '6',\n",
              " 'ft',\n",
              " ')',\n",
              " 'wide',\n",
              " ',',\n",
              " 'fifth']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJxcBjpN-N4F"
      },
      "source": [
        "temperature = 0.7\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WLOsMmi-NzF",
        "outputId": "e8674f95-a49c-4c2f-c77e-dd1e81482bf1"
      },
      "source": [
        "generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'highest',\n",
              " '<unk>',\n",
              " 'in',\n",
              " 'the',\n",
              " 'united',\n",
              " 'states',\n",
              " '.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'a',\n",
              " '<unk>',\n",
              " '@-@',\n",
              " '<unk>',\n",
              " 'and',\n",
              " 'a',\n",
              " '@-@',\n",
              " '<unk>',\n",
              " '@-@',\n",
              " 'chorus',\n",
              " 'sample',\n",
              " ',',\n",
              " 'which',\n",
              " 'features',\n",
              " 'the',\n",
              " '<unk>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}