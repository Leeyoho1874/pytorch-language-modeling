{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "awd_lstm_adam_regularized.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV9Cis3HdHp8",
        "outputId": "c84bd189-b3d7-425c-ff76-ad3f38bcf668"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.13)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaD4Nk85FsZc"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "\n",
        "import datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xZhIfJ9smN"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djyFDC4QFwVh",
        "outputId": "81089a44-cf8a-4bee-f551-1c56a4e77220"
      },
      "source": [
        "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKcltvAsfgHW"
      },
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aODmmThhcpTx"
      },
      "source": [
        "def tokenize_data(example, tokenizer):\n",
        "    tokens = {'tokens': tokenizer(example['text'])}\n",
        "    return tokens"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5x9lbSFc5Yh",
        "outputId": "179a6f8b-0001-4b56-b89c-244aa5086b15"
      },
      "source": [
        "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-f2efa5c633011b8b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-7f46f3973245ea2f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-3451e0738dc204d3.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1NIY8yiHJo"
      },
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],\n",
        "                                                  min_freq=3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54CbiCkw_5P3"
      },
      "source": [
        "vocab.insert_token('<unk>', 0)\n",
        "vocab.insert_token('<eos>', 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfSCMdZRioAG"
      },
      "source": [
        "vocab.get_itos()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grwHG5UN_eKN"
      },
      "source": [
        "unk_index = vocab['<unk>']\n",
        "vocab.set_default_index(unk_index)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUwATIrwkAi"
      },
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []\n",
        "    for example in dataset:\n",
        "        if example['tokens']:\n",
        "            tokens = example['tokens'].append('<eos>')\n",
        "            tokens = [vocab[token] for token in example['tokens']]\n",
        "            data.extend(tokens)\n",
        "    data = torch.LongTensor(data)\n",
        "    n_batches = data.shape[0] // batch_size\n",
        "    data = data.narrow(0, 0, n_batches * batch_size)\n",
        "    data = data.view(batch_size, -1)\n",
        "    return data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtjtIvuNisST"
      },
      "source": [
        "batch_size = 80\n",
        "\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
        "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnFp5L1XOzvc"
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq len, hidden dim]\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(x.shape[0], 1, x.shape[2], requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyXHwjATP8kU"
      },
      "source": [
        "def _setup_weight_drop(module, weights, dropout):\n",
        "    for name_w in weights:\n",
        "        w = getattr(module, name_w)\n",
        "        del module._parameters[name_w]\n",
        "        module.register_parameter(name_w + '_raw', nn.Parameter(w))\n",
        "\n",
        "    original_module_forward = module.forward\n",
        "\n",
        "    def forward(*args, **kwargs):\n",
        "        for name_w in weights:\n",
        "            raw_w = getattr(module, name_w + '_raw')\n",
        "            w = nn.Parameter(torch.nn.functional.dropout(raw_w, p=dropout, training=module.training))\n",
        "            setattr(module, name_w, w)\n",
        "\n",
        "        return original_module_forward(*args, **kwargs)\n",
        "\n",
        "    setattr(module, 'forward', forward)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JHLBu86RodS"
      },
      "source": [
        "class WeightDropLSTM(torch.nn.LSTM):\n",
        "    \"\"\"\n",
        "    Wrapper around :class:`torch.nn.LSTM` that adds ``weight_dropout`` named argument.\n",
        "\n",
        "    Args:\n",
        "        weight_dropout (float): The probability a weight will be dropped.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, weight_dropout=0.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        weights = ['weight_hh_l' + str(i) for i in range(self.num_layers)]\n",
        "        _setup_weight_drop(self, weights, weight_dropout)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAKBc-UUjO0-"
      },
      "source": [
        "class AWDLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, \n",
        "                 embedding_dropout_rate, weight_dropout_rate, lstm_dropout_rate, output_dropout_rate, \n",
        "                 tie_weights):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.tie_weights = tie_weights\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        lstms = []\n",
        "        for n in range(n_layers):\n",
        "            input_dim = embedding_dim if n == 0 else hidden_dim\n",
        "            output_dim = hidden_dim if n != n_layers - 1 else (embedding_dim if tie_weights else hidden_dim)\n",
        "            lstm = WeightDropLSTM(input_dim, output_dim, batch_first=True, weight_dropout=weight_dropout_rate)\n",
        "            lstms.append(lstm)\n",
        "        self.lstms = nn.ModuleList(lstms)\n",
        "        self.fc = nn.Linear(embedding_dim if tie_weights else hidden_dim, vocab_size)\n",
        "\n",
        "        self.embedding_dropout = LockedDropout(embedding_dropout_rate)\n",
        "        self.lstm_dropout = LockedDropout(lstm_dropout_rate)\n",
        "        self.output_dropout = LockedDropout(output_dropout_rate)\n",
        "\n",
        "        if tie_weights:\n",
        "            self.embedding.weight = self.fc.weight\n",
        "\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        init_range = 0.1\n",
        "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hiddens = []\n",
        "        for n in range(self.n_layers):\n",
        "            dim = self.hidden_dim if n != n_layers - 1 else (self.embedding_dim if self.tie_weights else self.hidden_dim)\n",
        "            hidden = torch.zeros(1, batch_size, dim).to(device)\n",
        "            cell = torch.zeros(1, batch_size, dim).to(device)\n",
        "            hiddens.append((hidden, cell))\n",
        "        return hiddens\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        if isinstance(hidden, torch.Tensor):\n",
        "            return hidden.detach()\n",
        "        else:\n",
        "            return tuple(self.detach_hidden(h) for h in hidden)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input = [batch size, seq len]\n",
        "        # hidden = list([1, batch size, hidden dim])\n",
        "        embedding = self.embedding_dropout(self.embedding(input))\n",
        "        # embedding = [batch size, seq len, embedding dim]\n",
        "        lstm_input = embedding\n",
        "        new_hiddens = []\n",
        "        for n, lstm in enumerate(self.lstms):\n",
        "            lstm_output, new_hidden = lstm(lstm_input, hidden[n])\n",
        "            # lstm_output = [batch size, seq len, hidden dim]\n",
        "            # new_hidden = [1, batch size, hidden dim]\n",
        "            if n != self.n_layers - 1:\n",
        "                lstm_output = self.lstm_dropout(lstm_output)\n",
        "            lstm_input = lstm_output\n",
        "            new_hiddens.append(new_hidden)\n",
        "        output = self.output_dropout(lstm_output)\n",
        "        prediction = self.fc(output)\n",
        "        # prediction = [batch size, seq len, vocab size]\n",
        "        # output = [batch size, seq len, hidden dim]\n",
        "        # lstm_output = [bath size, seq len, hidden dim]\n",
        "        # new_hiddens = list([1, batch size, hidden dim])\n",
        "        return prediction, output, lstm_output, new_hiddens"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYYhJ3WjWpY"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 400\n",
        "hidden_dim = 1150\n",
        "n_layers = 3\n",
        "embedding_dropout_rate = 0.65\n",
        "weight_dropout_rate = 0.5\n",
        "lstm_dropout_rate = 0.2\n",
        "output_dropout_rate = 0.4\n",
        "tie_weights = True\n",
        "\n",
        "model = AWDLSTM(vocab_size, embedding_dim, hidden_dim, n_layers,\n",
        "                embedding_dropout_rate, weight_dropout_rate, lstm_dropout_rate, output_dropout_rate, \n",
        "                tie_weights)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C9nTPpxsclE"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkggfxKJBZ04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a992721-b229-442c-bc31-f1c43f424cc9"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 32,030,273 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojbdsa4sfFS8"
      },
      "source": [
        "lr = 1e-3\n",
        "weight_decay = 1e-6\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv9GOZSgfGOj"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5PZPWeHfHyY",
        "outputId": "9e4f7a42-83aa-4b74-bcfc-72ecd31d461c"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q4ywswafKJT"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U6UG3_0fM8K"
      },
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, base_seq_len, alpha, beta, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    n_tokens = data.shape[-1]\n",
        "    base_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for input, target in get_batches(data, base_seq_len):\n",
        "        optimizer.zero_grad()\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "        # input = [batch size, seq len]\n",
        "        # target = [batch size, seq len]\n",
        "        batch_size, seq_len = input.shape\n",
        "        scaled_lr = base_lr * seq_len / base_seq_len\n",
        "        optimizer.param_groups[0][\"lr\"] = scaled_lr\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "        # hidden = list([1, batch size, hidden dim])\n",
        "        prediction, output, raw_output, hidden = model(input, hidden)\n",
        "        # prediction = [batch size, seq len, vocab size]\n",
        "        # output = [batch size, seq len, hidden dim]\n",
        "        # hidden = list([1, batch size, hidden dim])\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "        target = target.reshape(-1)\n",
        "        # output = [batch size * seq len, vocab size]\n",
        "        # target = [batch size * seq len]\n",
        "        loss = criterion(prediction, target)\n",
        "        alpha_loss = (alpha * output.pow(2).mean()).sum()\n",
        "        beta_loss = (beta * (output[:,1:] - output[:,:-1]).pow(2).mean()).sum()\n",
        "        loss = loss + alpha_loss + beta_loss \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / n_tokens"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIdNe9x4pkAw"
      },
      "source": [
        "def get_batches(data, seq_len):\n",
        "    data_len = data.shape[-1]\n",
        "    total_seq_len = 0\n",
        "    sampled_seq_lens = []\n",
        "    min_seq_len, max_seq_len = int(seq_len * 0.9), int(seq_len * 1.1)\n",
        "    while total_seq_len < data_len:\n",
        "        sampled_seq_len = random.randint(min_seq_len, max_seq_len)\n",
        "        sampled_seq_lens.append(sampled_seq_len)\n",
        "        total_seq_len += sampled_seq_len\n",
        "    sampled_seq_lens = sampled_seq_lens[:-1]\n",
        "    remainder = data_len - sum(sampled_seq_lens)\n",
        "    if remainder > min_seq_len:\n",
        "        sampled_seq_lens.append(remainder - 1)\n",
        "    pos = 0\n",
        "    for sampled_seq_len in sampled_seq_lens:\n",
        "        input = data[:,pos:pos+sampled_seq_len]\n",
        "        target = data[:,pos+1:pos+sampled_seq_len+1]\n",
        "        pos += sampled_seq_len\n",
        "        yield input, target"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMfUy_H2IdUo"
      },
      "source": [
        "def evaluate(model, data, criterion, batch_size, base_seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    n_tokens = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input, target in get_batches(data, base_seq_len):\n",
        "            input = input.to(device)\n",
        "            target = target.to(device)\n",
        "            # input = [batch size, seq len]\n",
        "            # target = [batch size, seq len]\n",
        "            batch_size, seq_len = input.shape\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            # hidden = list([1, batch size, hidden dim])\n",
        "            prediction, _, _, hidden = model(input, hidden)\n",
        "            # prediction = [batch size, seq len, vocab size]\n",
        "            # hidden = list([1, batch size, hidden dim])\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "            # prediction = [batch size * seq len, vocab size]\n",
        "            # target = [batch size * seq len]\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / n_tokens"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keYcu0DthiJ0"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoPhYH3PJB-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ad4100-c267-4ed2-dc77-83d0786c50a6"
      },
      "source": [
        "n_epochs = 50\n",
        "seq_len = 70\n",
        "clip = 0.25\n",
        "alpha = 2\n",
        "beta = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, alpha, beta, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
        "    \n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'awd-lstm_lm.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 956.350\n",
            "\tValid Perplexity: 372.991\n",
            "Epoch: 02 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 459.650\n",
            "\tValid Perplexity: 269.607\n",
            "Epoch: 03 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 342.260\n",
            "\tValid Perplexity: 217.015\n",
            "Epoch: 04 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 272.380\n",
            "\tValid Perplexity: 174.195\n",
            "Epoch: 05 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 228.770\n",
            "\tValid Perplexity: 152.187\n",
            "Epoch: 06 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 202.017\n",
            "\tValid Perplexity: 143.913\n",
            "Epoch: 07 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 178.614\n",
            "\tValid Perplexity: 127.991\n",
            "Epoch: 08 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 165.137\n",
            "\tValid Perplexity: 130.775\n",
            "Epoch: 09 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 151.638\n",
            "\tValid Perplexity: 126.324\n",
            "Epoch: 10 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 142.756\n",
            "\tValid Perplexity: 117.159\n",
            "Epoch: 11 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 133.374\n",
            "\tValid Perplexity: 116.873\n",
            "Epoch: 12 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 128.099\n",
            "\tValid Perplexity: 111.996\n",
            "Epoch: 13 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 122.446\n",
            "\tValid Perplexity: 107.053\n",
            "Epoch: 14 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 117.852\n",
            "\tValid Perplexity: 108.180\n",
            "Epoch: 15 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 112.488\n",
            "\tValid Perplexity: 96.125\n",
            "Epoch: 16 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 109.241\n",
            "\tValid Perplexity: 104.615\n",
            "Epoch: 17 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 104.451\n",
            "\tValid Perplexity: 100.776\n",
            "Epoch: 18 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 101.584\n",
            "\tValid Perplexity: 97.502\n",
            "Epoch: 19 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 98.065\n",
            "\tValid Perplexity: 93.805\n",
            "Epoch: 20 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 95.723\n",
            "\tValid Perplexity: 97.801\n",
            "Epoch: 21 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 93.230\n",
            "\tValid Perplexity: 94.290\n",
            "Epoch: 22 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 90.846\n",
            "\tValid Perplexity: 94.152\n",
            "Epoch: 23 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 88.562\n",
            "\tValid Perplexity: 95.342\n",
            "Epoch: 24 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 86.526\n",
            "\tValid Perplexity: 94.128\n",
            "Epoch: 25 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 84.245\n",
            "\tValid Perplexity: 87.928\n",
            "Epoch: 26 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 82.425\n",
            "\tValid Perplexity: 84.185\n",
            "Epoch: 27 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 80.868\n",
            "\tValid Perplexity: 86.398\n",
            "Epoch: 28 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 79.657\n",
            "\tValid Perplexity: 85.473\n",
            "Epoch: 29 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 79.000\n",
            "\tValid Perplexity: 83.720\n",
            "Epoch: 30 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 78.145\n",
            "\tValid Perplexity: 90.629\n",
            "Epoch: 31 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 76.657\n",
            "\tValid Perplexity: 83.256\n",
            "Epoch: 32 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 75.238\n",
            "\tValid Perplexity: 81.607\n",
            "Epoch: 33 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 75.011\n",
            "\tValid Perplexity: 89.251\n",
            "Epoch: 34 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 74.425\n",
            "\tValid Perplexity: 83.179\n",
            "Epoch: 35 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 73.514\n",
            "\tValid Perplexity: 83.498\n",
            "Epoch: 36 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 72.170\n",
            "\tValid Perplexity: 88.532\n",
            "Epoch: 37 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 71.290\n",
            "\tValid Perplexity: 79.819\n",
            "Epoch: 38 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 70.005\n",
            "\tValid Perplexity: 86.469\n",
            "Epoch: 39 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 69.516\n",
            "\tValid Perplexity: 82.408\n",
            "Epoch: 40 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 68.248\n",
            "\tValid Perplexity: 83.579\n",
            "Epoch: 41 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 68.200\n",
            "\tValid Perplexity: 79.714\n",
            "Epoch: 42 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 67.798\n",
            "\tValid Perplexity: 86.446\n",
            "Epoch: 43 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 66.893\n",
            "\tValid Perplexity: 84.040\n",
            "Epoch: 44 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 66.680\n",
            "\tValid Perplexity: 83.059\n",
            "Epoch: 45 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 66.351\n",
            "\tValid Perplexity: 80.735\n",
            "Epoch: 46 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 64.995\n",
            "\tValid Perplexity: 79.748\n",
            "Epoch: 47 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 64.341\n",
            "\tValid Perplexity: 78.305\n",
            "Epoch: 48 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 63.978\n",
            "\tValid Perplexity: 78.904\n",
            "Epoch: 49 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 63.607\n",
            "\tValid Perplexity: 81.596\n",
            "Epoch: 50 | Epoch Time: 1m 27s\n",
            "\tTrain Perplexity: 63.424\n",
            "\tValid Perplexity: 85.184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9gMl22AweGP",
        "outputId": "39dd372a-4fbc-4633-e48b-e8341ae5f5f5"
      },
      "source": [
        "model.load_state_dict(torch.load('awd-lstm_lm.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Perplexity: 80.583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ypQ5_huwmcf"
      },
      "source": [
        "def generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(0)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_gen_tokens):\n",
        "            input = torch.LongTensor([indices]).to(device)\n",
        "            prediction, _, _, hidden = model(input, hidden)\n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1) \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "            indices.append(prediction)\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUBkgspjwn_J",
        "outputId": "8106a62a-04e1-4e19-ac47-db5853f36bd7"
      },
      "source": [
        "prompt = 'the'\n",
        "n_gen_tokens = 25\n",
        "temperature = 0.5\n",
        "seed = 0\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqbGquhWwpSc",
        "outputId": "137a67e4-4fcb-4a80-832f-7ee2bab3c190"
      },
      "source": [
        "generation"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " '<unk>',\n",
              " '<unk>',\n",
              " ',',\n",
              " 'which',\n",
              " 'was',\n",
              " 'the',\n",
              " 'first',\n",
              " 'to',\n",
              " 'be',\n",
              " 'named',\n",
              " 'the',\n",
              " '<unk>',\n",
              " '.',\n",
              " '<eos>',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " 'death',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '<eos>',\n",
              " 'the',\n",
              " 'first',\n",
              " '@-@']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZrBvc5vwrth",
        "outputId": "9aa0c94e-3162-4ad0-908e-f254f4773781"
      },
      "source": [
        "temperature = 0.1\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4HKOCKywsL0",
        "outputId": "d14ae4dc-887d-406d-c98e-da31c95a3c92"
      },
      "source": [
        "generation"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " '<unk>',\n",
              " '<unk>',\n",
              " '.',\n",
              " '<eos>',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '<unk>',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '=',\n",
              " '<eos>',\n",
              " 'the',\n",
              " '<unk>',\n",
              " '<unk>',\n",
              " '<unk>',\n",
              " '(',\n",
              " '<unk>',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " '<unk>',\n",
              " '<unk>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTnFqFZywthG",
        "outputId": "629476f7-2a6b-4cc5-f80b-a7a40773a126"
      },
      "source": [
        "temperature = 1.5\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhAnRwinwtan",
        "outputId": "51b4c64e-0dd7-4e02-ff60-e2276bf83312"
      },
      "source": [
        "generation"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'rigging',\n",
              " 'swap',\n",
              " '228',\n",
              " 'wansel',\n",
              " 'and',\n",
              " 'protestants',\n",
              " 'arranged',\n",
              " 'discussions',\n",
              " '3',\n",
              " 'agree',\n",
              " 'oldman',\n",
              " 'ctesiphon',\n",
              " ',',\n",
              " 'blown',\n",
              " 'harvest',\n",
              " 'manny',\n",
              " 'friday',\n",
              " 'the',\n",
              " 'tom',\n",
              " 'sample',\n",
              " 'giger',\n",
              " 'viewed',\n",
              " 'accommodated',\n",
              " '138',\n",
              " 'paces']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8UvBRy1wtUg",
        "outputId": "883e7288-fdbe-4feb-86d3-b4609a0eab3e"
      },
      "source": [
        "temperature = 0.75\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deEaUs6owtKt",
        "outputId": "ef6fce5a-8221-4a9c-87f7-d7f6ae654a3b"
      },
      "source": [
        "generation"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'rigging',\n",
              " 'of',\n",
              " 'the',\n",
              " 'hell',\n",
              " '.',\n",
              " 'the',\n",
              " 'general',\n",
              " \"'\",\n",
              " 's',\n",
              " 'movement',\n",
              " 'was',\n",
              " 'a',\n",
              " 'theme',\n",
              " 'of',\n",
              " 'the',\n",
              " 'jin',\n",
              " \"'\",\n",
              " 's',\n",
              " 'tom',\n",
              " '<unk>',\n",
              " ',',\n",
              " 'who',\n",
              " 'gathered',\n",
              " 'the',\n",
              " 'invading']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBh_zvL1ws7c",
        "outputId": "2d18f1db-6d5e-4c68-ba2b-287dc6d08111"
      },
      "source": [
        "temperature = 0.8\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k93j5Kbfw7qk",
        "outputId": "571712de-1655-4cb6-ca0c-64aea6e8925c"
      },
      "source": [
        "generation"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'rigging',\n",
              " '<unk>',\n",
              " 'the',\n",
              " 'hell',\n",
              " '.',\n",
              " 'the',\n",
              " 'general',\n",
              " \"'\",\n",
              " 's',\n",
              " 'movement',\n",
              " 'was',\n",
              " 'later',\n",
              " 'retired',\n",
              " 'and',\n",
              " 'the',\n",
              " 'jin',\n",
              " 'captains',\n",
              " 'had',\n",
              " 'lost',\n",
              " 'the',\n",
              " 'opportunity',\n",
              " 'to',\n",
              " 'deliver',\n",
              " 'the',\n",
              " 'invading']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9fS58ldw8rV",
        "outputId": "2a886169-4970-4f83-c989-94cc56123a84"
      },
      "source": [
        "temperature = 0.7\n",
        "\n",
        "generation = generate(prompt, n_gen_tokens, temperature, model, tokenizer, vocab, device, seed)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:680: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:924.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IwhwPhyw8m_",
        "outputId": "edac3031-6157-4abd-a8d1-3ce124d4d517"
      },
      "source": [
        "generation"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'rigging',\n",
              " 'of',\n",
              " 'the',\n",
              " 'hell',\n",
              " '.',\n",
              " 'the',\n",
              " 'general',\n",
              " \"'\",\n",
              " 's',\n",
              " 'plan',\n",
              " 'was',\n",
              " 'to',\n",
              " 'be',\n",
              " 'blown',\n",
              " 'over',\n",
              " 'as',\n",
              " 'the',\n",
              " 'embattled',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'rear',\n",
              " ',',\n",
              " 'and',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}